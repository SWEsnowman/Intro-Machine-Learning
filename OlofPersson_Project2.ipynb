{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Olof Persson\n",
    "#Project 2 - Neural Network Grid Search Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as cPickle, gzip\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f,encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE NEURAL NETWORK - this is gradient descent with batching to the data to create a more accurate gradient\n",
    "def train_nn(X, y, hidden_dim, learning_rate, reg_strength, num_iters):\n",
    "#     Train a neural network on the given data.\n",
    "#     X (numpy.ndarray): Input data, shape (N, D).\n",
    "#     y (numpy.ndarray): Labels, shape (N,).\n",
    "#     hidden_dim (int): Number of neurons in the hidden layer.\n",
    "#     learning_rate (float): Learning rate for gradient descent.\n",
    "#     reg_strength (float): Regularization strength.\n",
    "#     num_iters (int): Number of iterations for gradient descent.\n",
    "    batch_size = 1000\n",
    "    num_classes = len(np.unique(y))\n",
    "    num_examples, input_dim = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim / 2)\n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    W2 = np.random.randn(hidden_dim, num_classes) / np.sqrt(hidden_dim / 2)\n",
    "    b2 = np.zeros((1, num_classes))\n",
    "\n",
    "    # Gradient descent loop\n",
    "    loss_history = []\n",
    "    for i in range(num_iters):\n",
    "            # Shuffle the data after every epoch\n",
    "            shuffled_indices = np.random.permutation(num_examples)\n",
    "            x_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            \n",
    "            for i in range(0, num_examples, batch_size):\n",
    "        # Forward pass\n",
    "                z1 = np.dot(X, W1) + b1\n",
    "                a1 = sigmoid(z1)\n",
    "                z2 = np.dot(a1, W2) + b2\n",
    "                scores = sigmoid(z2)\n",
    "\n",
    "        # Compute loss and regularization\n",
    "                data_loss = np.mean(-np.log(scores[range(num_examples), y]))\n",
    "                reg_loss = 0.5 * reg_strength * (np.sum(W1 * W1) + np.sum(W2 * W2))\n",
    "                loss = data_loss + reg_loss\n",
    "                loss_history.append(loss)\n",
    "\n",
    "        # Backward pass\n",
    "                dscores = scores\n",
    "                dscores[range(num_examples), y] -= 1\n",
    "                dscores /= num_examples\n",
    "\n",
    "                dW2 = np.dot(a1.T, dscores)\n",
    "                db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "                da1 = np.dot(dscores, W2.T)\n",
    "                dz1 = da1 * sigmoid_derivative(a1)\n",
    "\n",
    "                dW1 = np.dot(X.T, dz1)\n",
    "                db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # Add regularization\n",
    "                dW2 += reg_strength * W2\n",
    "                dW1 += reg_strength * W1\n",
    "\n",
    "        # Perform parameter update\n",
    "                W1 -= learning_rate * dW1\n",
    "                b1 -= learning_rate * db1\n",
    "                W2 -= learning_rate * dW2\n",
    "                b2 -= learning_rate * db2\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    weights = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "    return weights, loss_history[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, weights, biases):\n",
    "    activations = test_data\n",
    "    for i in range(len(weights)):\n",
    "        activations = np.dot(activations, weights[i]) + biases[i]\n",
    "        activations = sigmoid(activations)\n",
    "    predicted_labels = np.argmax(activations, axis=1)\n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with hidden size:  64  and learning rate:  0.001\n",
      "Loss accuracy: 1.872101641603634\n",
      "Done with hidden size:  128  and learning rate:  0.001\n",
      "Loss accuracy: 2.0818422159091616\n",
      "Done with hidden size:  64  and learning rate:  0.01\n",
      "Loss accuracy: 1.8341151031556326\n",
      "Done with hidden size:  128  and learning rate:  0.01\n",
      "Loss accuracy: 1.7969256283495365\n",
      "Test accuracy: 0.4028\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Divide the data into training, validation, and test set x and y values\n",
    "X_train, y_train = train_set\n",
    "X_val, y_val = valid_set\n",
    "X_test, y_test = test_set\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "learning_rates = [0.001, 0.01]\n",
    "hidden_size_sets = [64, 128]\n",
    "\n",
    "best_loss_acc = 10000000\n",
    "best_hyperparams = {}\n",
    "# Loop over all hyperparameter combinations and evaluate validation accuracy\n",
    "# TASK 1 the grid search and the calling of the neural network in each pass of the gird search\n",
    "# THE GRID SEARCH ALGORITHM - it is simplified with less hyper parameters to search through because it already takes a super long time to run\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hidden_sizes in hidden_size_sets:\n",
    "        #model, val_acc = train_nn1(X_train, y_train, hidden_sizes1, output_size1, lr)\n",
    "        model, loss_acc = train_nn(X_train, y_train,hidden_sizes, lr, 0, 10)\n",
    "        print(\"Done with hidden size: \", hidden_sizes, \" and learning rate: \", lr)\n",
    "        print('Loss accuracy:', loss_acc)\n",
    "        if loss_acc < best_loss_acc:\n",
    "            best_loss_acc = loss_acc\n",
    "            best_hyperparams = {'lr': lr, 'hidden_sizes': hidden_sizes}\n",
    "\n",
    "# TASK 2 the training of the neural network with the best hyper parameters and predicting the test set and then reporting the accuracy\n",
    "\n",
    "# Train a new model with the best hyperparameters on the combined training and validation sets\n",
    "best_model, val_acc = train_nn(X_val, y_val, best_hyperparams['hidden_sizes'], best_hyperparams['lr'], 0, 10)\n",
    "y_pred = predict(X_test, [best_model['W1'], best_model['W2']], [best_model['b1'], best_model['b2']])\n",
    "# Test the performance of the model on the test set\n",
    "test_acc = np.mean(y_pred == y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The example above was done with the number of iterations down to 10 vs the 100 used to get the 70% accuracy on\n",
    "#single neural network tests mentioned below, becuase with 10 iterations it took 27 minutes to run the grid search on\n",
    "#the computer. That is why the accuracy is so low. The accuracy with 100 iterations is 70% and more. I hope that is example \n",
    "#showing the grid search is enough to show that the grid search works and such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing to Assignment 3, Question 2\n",
    "#The result compared to question 2 in assignment 3 is a little bit worse in that this one has around a 70-75% accuracy, \n",
    "#whereas the gridsearchCV algorithm and the MLPclassifier in the sklearn package resulted in a 96% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discuss my performance\n",
    "#The MLPclassifier only had one hidden layer like my algorithm, but the algorithm is definitely more accurate. \n",
    "#One way that I could improve the accuracy of my algorithm is to add more hidden layers, but that would take a lot more \n",
    "#time to run and increase the complexity of the algorithm. I could also try to add more hyper parameters to search through\n",
    "#in the grid search algorithm. Something else I could also do would be to change the batch size in the gradient descent to\n",
    "#be smaller or larger depending what makes the algorithm give a greater accuracy even if the smaller batch size takes longer.\n",
    "#Lastly, I can think about tuning the regularization strength to see if the algorithm needed to be adjusted for overfitting. \n",
    "#All of those options could increase the accuracy of the algorithm, but it could make it take \n",
    "#a lot longer to run on an algorithm that already times a lot of time to run compared to the built in MLPclassifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
